{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "   In order to investigate the impacts of emotional states on our stock model, we would first need to generate a sentiment scores of an observed company have news published on newspaper as well as stock prices announced on a given day. \n",
    "\n",
    "## Stock Classification \n",
    "<img width=\"669\" alt=\"screenshot 2018-11-12 18 41 45\" src=\"https://user-images.githubusercontent.com/30711638/48381597-a745d300-e6aa-11e8-8762-04c34c2f7569.png\">\n",
    "\n",
    "## Data Description\n",
    "Our final dataset is the combination of historical stock data that reflects the pattern of stock movements, fundamental parameters that indicate the long-term financial health of a company, and the sentiment scores that symbolize the public opinions towards the given company.\n",
    "### 1. Financial quantitative data\n",
    "There are 2 sources of financial data that we use in the analysis\n",
    "-Daily historical stock price that we get from [Yahoo Finance](https://finance.yahoo.com/quote/AAPL/history?p=AAPL)\n",
    "-The fundamental information of a company’s stocks, which has 198 features including figures such as  debt, equity, book values, etc. We get this data from [GuruFocus](https://www.gurufocus.com/term/Shares+Outstanding/AAPL/Shares-Outstanding-Diluted-Average/Apple-Inc)\n",
    "### 2. Financial news\n",
    "We obtained financial news data by scraping articles related to 21 popular companies that have regularly appeared on the news from 01/01/2011 to the present. Although this was supposed to give us approximately 45,000 instances of various news events, there are many days where the observed companies don’t have any news, so after dropping all of the days with no news, we were left with a sample size of approximately 25,000 instances.  \n",
    "\n",
    "In this format, wi=1 if word i is in the given piece of news. Then, we fed the set of feature vectors through multiple machine learning algorithms to determine the optimal classifier for generating sentiment scores. We refed each news event through this classifier to generate a corresponding sentiment score, which would be treated as a new feature for our stock classification model.\n",
    "\n",
    "## Generating sentiment scores on financial news\n",
    "<img width=\"732\" alt=\"screenshot 2018-11-12 18 43 47\" src=\"https://user-images.githubusercontent.com/30711638/48381638-e8d67e00-e6aa-11e8-8b37-85c49f87ed68.png\">\n",
    "\n",
    "In order to classify a news event with positive or negative sentiment, we used the Bags of Words approach, in which a piece of news is converted to a feature vector (w1,., wn) consisting of words.\n",
    "\n",
    "## Preprocessing Techniques\n",
    "To simplify the process and reduce computation time, we treated all news published on any given day as a single piece of news. By doing this, we assumed that all news published in a day about a company has the same sentiment.\n",
    "<img width=\"227\" alt=\"screenshot 2018-11-12 22 24 45\" src=\"https://user-images.githubusercontent.com/30711638/48389049-c6a02880-e6c9-11e8-8204-f54155bc4d69.png\">\n",
    "\n",
    "We applied many common NLP techniques to clean our news data\n",
    "\n",
    "- Remove:\n",
    "    - Punctuations\n",
    "    - Stop words\n",
    "    - Tokenize\n",
    "    - Any words that don’t appear at least 2 times throughout our dataset because these features aren’t likely to reveal any patterns\n",
    "- Tokenizing text in each given event date:\n",
    "    - Convert uppercase to lowercase\n",
    "    - Lemmatizing: convert words into the corresponding words\n",
    "    - Word Tagging: determine word types. In this project, We decided to only keep adjectives, adverbs and verbs for our feature vectors because nouns generally don’t reveal much sentiment information\n",
    "    - Bigram: consider multiple words together\n",
    "- Ex: “Apple set to expand Siri, taking different route from Amazons Alexa”\n",
    "    - Vectors of Words = [‘set’, ‘expand’, ‘take’, ‘different’]\n",
    "We also conducted many pair tests for each preprocessing techniques to determine what preprocessing techniques, when combined, improves the performance of our classification\n",
    "\n",
    "<img width=\"602\" alt=\"screenshot 2018-11-12 19 17 03\" src=\"https://user-images.githubusercontent.com/30711638/48382551-9481cd00-e6af-11e8-9fc9-07a8a07d8e54.png\">\n",
    "\n",
    "Based on table 1, which plots the accuracy of the Naive Bayes model, we observe that none of the preprocessing techniques significantly improves the performance of the classification. We also notice that Stemming, which reduces words to roots of words, isn’t as informative as Lemmatizing since the roots of words may not have been used in the actual articles. That’s why we need to be careful applying the different techniques that are commonly used for machine learning problems: the techniques might degrade instead improve the model performance if it’s not suited for our data. It’s also commonly known that bigram might degrade our classification as it increases our features size without adding more useful information.  Table 1 shows that the combination of bigram, lemmatizing, and word tagging works the best. However, Table 1 also shows that the accuracy of most algorithms is very low, almost equivalent to random guessing. We think that this is due to the curse of dimensionality, since we have lots of features with only a sample size of 1254 and a feature size of 3455, worsening the sparseness of the data. Next, we want to investigate how the performance of text classification improves with increasing sample size.\n",
    "\n",
    "# Experiments\n",
    "## Building text classifiers for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the neccessary packages\n",
    "import pandas as pd \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.classify import ClassifierI\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pickle \n",
    "import pdb\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import itertools\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "from nltk.metrics.scores import accuracy, precision, recall\n",
    "import collections\n",
    "import os, sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tjhuynh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/tjhuynh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tjhuynh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tjhuynh/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Because we have to process news articles for 21 different companies, it's better to write functions to read files of each company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read news files of a company\n",
    "def create_news_df(news_file):\n",
    "    news_df = pd.read_csv(news_file, encoding = \"ISO-8859-1\")\n",
    "    news_df['Date'] = pd.to_datetime(news_df['Date'], errors= 'coerce')\n",
    "    news_df['Text'] = news_df['Body'].astype(str).str.cat(news_df['Title'].astype(str))\n",
    "    del news_df['Body']\n",
    "    del news_df['Title']\n",
    "    news_df.fillna('', inplace=True)\n",
    "    return news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create labels for text classfication\n",
    "\n",
    "<img width=\"341\" alt=\"screenshot 2018-11-12 22 35 11\" src=\"https://user-images.githubusercontent.com/30711638/48389388-3bc02d80-e6cb-11e8-84ca-80edc5db2a8d.png\">\n",
    "\n",
    "To perform any classification task, I need to have labels for our data. Because I wanted to investigate how the news related to the stock movements, I decided that I should use some indicators of stock as the labels for the sentiment analysis model. One common binary stock indicator is comparing the stock returns with the return of S&P 500 index. This is a considerable assumption since\n",
    "\n",
    "- The stock price is determined by aggregating the trades of everyone who reacts to the news, as well as the trades of other agents whose decisions are independent of what is in the news.\n",
    "- The CAMP model states that a stock return can be decomposed into two components:\n",
    "\n",
    "    - One captures all the risks correlated with the market\n",
    "    - One captures all the risks that is unique to the stock. In the other words, it represents the well-being of the company itself and is independent of market movements and other assets.\n",
    "These factors increases the uncertainties about the stock prediction, and in order to remedy this, I seek to find a quantity that is independent of the risk in market movements and other information not related to the news event. I found out that the quanity **Abnormal Return** satisfies the desired characteristics for stock label instead of the S&P 500.\n",
    "\n",
    "\n",
    "## Method 1\n",
    "## Label: Abnormal Return Changes\n",
    "[Abnormal Return](https://www.investopedia.com/terms/a/abnormalreturn.asp) is the difference between the actual return of a security over a period of time and the expected return. The expected rate of return is the estimated return based on an asset pricing model, using a long run historical average or multiple valuation.\n",
    "The thesis presented by Pablo Daniel Azar gives a much thorough understanding of Abnormal Return in the context of financial analysis.\n",
    "\n",
    "Mathematically, Abnormal Return can be expressed as:\n",
    "\n",
    "`Abnormal return = expected return - actual return (1)`\n",
    "\n",
    "The \"actual return\" is the return that I obtained from Yahoo Finance and the \"expected return\" refers to the forecast return calculated by the Capital Asset Pricing Model (CAMP) framework. The basic formular for calculating a stock's expected return under the CAMP is\n",
    "\n",
    "`Expected return = risk-free rate + beta x (market return - risk-free rate) (2)`\n",
    "\n",
    "Using the (1) and (2) formulars, I calculate the abnomal return for 6-year time period of 22 companies and save the results in the folder Abnormal Returns. I read these \"Abnormal Returns\" csv file into dataframe with function `create_stock_df`.\n",
    "\n",
    "To generate binary label, we create a threshold to convert Abnomral Return from continuous to binary values. \n",
    "<img width=\"504\" alt=\"screenshot 2018-11-12 19 14 37\" src=\"https://user-images.githubusercontent.com/30711638/48382452-36ed8080-e6af-11e8-97a7-aaefc285efd6.png\">\n",
    "\n",
    "- (+): Abnormal Return > 0.01%\n",
    "- (-): Abnormal Return < 0.01%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Yahoo stock of a company\n",
    "def create_stock_df(stock_file):\n",
    "    #  append Target into News dataframe\n",
    "    stock_df = pd.read_csv(stock_file, encoding = \"ISO-8859-1\")\n",
    "    stock_df['Date'] = pd.to_datetime(stock_df['Date'])\n",
    "    return stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine news data and stock data of a company to prepare for label creation process\n",
    "def combine_final_df(news_df, stock_df):\n",
    "    df= stock_df.set_index('Date').join(news_df.set_index('Date'))\n",
    "    df.fillna(0, inplace = True)\n",
    "    df['Target'] = np.nan\n",
    "    print(df.head())\n",
    "    requirement = 0.00000\n",
    "    for i in range(len(df)):\n",
    "        if df['Abnormal Return'].iloc[i] > requirement:\n",
    "            df['Target'].iloc[i] = 1.0\n",
    "        elif df['Abnormal Return'].iloc[i] <  -requirement:\n",
    "            df['Target'].iloc[i] = -1.0\n",
    "        else:\n",
    "            df['Target'].iloc[i] = 0.0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I make an assumption that the return on a firm is directly connected to the news. This is a relatively big assumption because in financial news, the text is written by a reporter who has no control over how the market will react. The classification is assigned by aggregating the trades of everyone who reacts to the news, as well as the trades of other agents whose decisions are independent of what is in the news. However, this assumption is reasonable for our model in which we try to investigate the relationship between the financial news and stock performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We conduct experiments with data of 21 companies, so we have to combine processed data of these companies together to create 1 complete training dataset\n",
    "def combine_multiple_company(company_list):  \n",
    "    \"\"\"\n",
    "    Concatenate multiple dataframe of companies together to create big lexicon\n",
    "    \"\"\"\n",
    "    company_dfs = []\n",
    "\n",
    "    for company in company_list:\n",
    "        print(company)\n",
    "        news_file_name = 'data/News/'+ company + '_News.csv'\n",
    "        news = create_news_df(news_file_name)\n",
    "        stock_file_name = 'data/Abnormal_Returns/' + company + '_AbnormalReturn.csv'\n",
    "        stock = create_stock_df(stock_file_name)\n",
    "\n",
    "        final = combine_final_df(news, stock)\n",
    "        company_dfs.append(final)\n",
    "    total = pd.concat(company_dfs, ignore_index = True)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "005930.KS\n",
      "            Abnormal Return  \\\n",
      "Date                          \n",
      "2012-01-03           0.0000   \n",
      "2012-01-04           0.0000   \n",
      "2012-01-05          -0.0001   \n",
      "2012-01-06           0.0001   \n",
      "2012-01-09          -0.0001   \n",
      "\n",
      "                                                         Text  Target  \n",
      "Date                                                                   \n",
      "2012-01-03  SEOUL South Korea said on Wednesday it had app...     NaN  \n",
      "2012-01-04  SEOUL, Jan 4 Seoul shares slipped on\\rWednesda...     NaN  \n",
      "2012-01-05  SEOUL Samsung Electronics, the world's top mak...     NaN  \n",
      "2012-01-06  TAIPEI Taiwanese smartphone maker HTC Corp rec...     NaN  \n",
      "2012-01-09  LAS VEGAS Samsung Electronics Co unveiled its ...     NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tjhuynh/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL\n",
      "            Abnormal Return    Text  Target\n",
      "Date                                       \n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-04         0.006427  nannan     NaN\n",
      "2011-01-05         0.003572  nannan     NaN\n",
      "2011-01-06         0.001145  nannan     NaN\n",
      "2011-01-07         0.008859  nannan     NaN\n",
      "INTC\n",
      "            Abnormal Return    Text  Target\n",
      "Date                                       \n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-04         0.015676  nannan     NaN\n",
      "2011-01-05        -0.014839  nannan     NaN\n",
      "2011-01-06        -0.006037  nannan     NaN\n",
      "2011-01-07        -0.003487  nannan     NaN\n",
      "MSFT\n",
      "            Abnormal Return    Text  Target\n",
      "Date                                       \n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-04         0.005247  nannan     NaN\n",
      "2011-01-05        -0.008219  nannan     NaN\n",
      "2011-01-06         0.031412  nannan     NaN\n",
      "2011-01-07        -0.005786  nannan     NaN\n",
      "ORCL\n",
      "            Abnormal Return    Text  Target\n",
      "Date                                       \n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-04        -0.002939  nannan     NaN\n",
      "2011-01-05        -0.019653  nannan     NaN\n",
      "2011-01-06         0.006595  nannan     NaN\n",
      "2011-01-07        -0.002400  nannan     NaN\n",
      "SNE\n",
      "            Abnormal Return    Text  Target\n",
      "Date                                       \n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-04        -0.004478  nannan     NaN\n",
      "2011-01-05        -0.004602  nannan     NaN\n",
      "2011-01-06        -0.002986  nannan     NaN\n",
      "2011-01-07         0.003038  nannan     NaN\n",
      "TDC\n",
      "            Abnormal Return    Text  Target\n",
      "Date                                       \n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-04        -0.011958  nannan     NaN\n",
      "2011-01-05         0.010563  nannan     NaN\n",
      "2011-01-06         0.019781  nannan     NaN\n",
      "2011-01-07         0.018681  nannan     NaN\n",
      "TSLA\n",
      "            Abnormal Return    Text  Target\n",
      "Date                                       \n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-04         0.003577  nannan     NaN\n",
      "2011-01-05        -0.000480  nannan     NaN\n",
      "2011-01-06         0.041883  nannan     NaN\n",
      "2011-01-07         0.015300  nannan     NaN\n",
      "TXN\n",
      "            Abnormal Return    Text  Target\n",
      "Date                                       \n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-04        -0.000344  nannan     NaN\n",
      "2011-01-05        -0.001699  nannan     NaN\n",
      "2011-01-06         0.016127  nannan     NaN\n",
      "2011-01-07         0.001190  nannan     NaN\n",
      "FB\n",
      "            Abnormal Return    Text  Target\n",
      "Date                                       \n",
      "2012-05-18         0.000000  nannan     NaN\n",
      "2012-05-21        -0.126478  nannan     NaN\n",
      "2012-05-22        -0.089543  nannan     NaN\n",
      "2012-05-23         0.030503  nannan     NaN\n",
      "2012-05-24         0.030757  nannan     NaN\n",
      "AMZN\n",
      "            Abnormal Return    Text  Target\n",
      "Date                                       \n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-04         0.005729  nannan     NaN\n",
      "2011-01-05         0.007532  nannan     NaN\n",
      "2011-01-06        -0.005994  nannan     NaN\n",
      "2011-01-07         0.000034  nannan     NaN\n",
      "QCOM\n",
      "            Abnormal Return    Text  Target\n",
      "Date                                       \n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-04         0.017156  nannan     NaN\n",
      "2011-01-05         0.015409  nannan     NaN\n",
      "2011-01-06         0.014585  nannan     NaN\n",
      "2011-01-07        -0.015862  nannan     NaN\n",
      "GOOG.O\n",
      "            Abnormal Return    Text  Target\n",
      "Date                                       \n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-04        -0.002418  nannan     NaN\n",
      "2011-01-05         0.006693  nannan     NaN\n",
      "2011-01-06         0.009329  nannan     NaN\n",
      "2011-01-07         0.006579  nannan     NaN\n",
      "IBM\n",
      "            Abnormal Return    Text  Target\n",
      "Date                                       \n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-04         0.002173  nannan     NaN\n",
      "2011-01-05        -0.008143  nannan     NaN\n",
      "2011-01-06         0.012707  nannan     NaN\n",
      "2011-01-07        -0.003382  nannan     NaN\n",
      "CVX\n",
      "            Abnormal Return    Text  Target\n",
      "Date                                       \n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-04        -0.002314  nannan     NaN\n",
      "2011-01-04        -0.002314  nannan     NaN\n",
      "GE\n",
      "            Abnormal Return    Text  Target\n",
      "Date                                       \n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-04         0.019447  nannan     NaN\n",
      "2011-01-05        -0.003708  nannan     NaN\n",
      "2011-01-06        -0.002037  nannan     NaN\n",
      "2011-01-07        -0.005044  nannan     NaN\n",
      "WMT\n",
      "            Abnormal Return    Text  Target\n",
      "Date                                       \n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-04         0.004527  nannan     NaN\n",
      "2011-01-04         0.004527  nannan     NaN\n",
      "2011-01-05        -0.009157  nannan     NaN\n",
      "WFC\n",
      "            Abnormal Return    Text  Target\n",
      "Date                                       \n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-04         0.003873  nannan     NaN\n",
      "2011-01-04         0.003873  nannan     NaN\n",
      "XOM\n",
      "            Abnormal Return    Text  Target\n",
      "Date                                       \n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-04         0.005922  nannan     NaN\n",
      "2011-01-05        -0.007349  nannan     NaN\n",
      "2011-01-06         0.008409  nannan     NaN\n",
      "2011-01-07         0.007178  nannan     NaN\n",
      "T\n",
      "            Abnormal Return    Text  Target\n",
      "Date                                       \n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-04         0.007889  nannan     NaN\n",
      "2011-01-04         0.007889  nannan     NaN\n",
      "2011-01-05         0.000255  nannan     NaN\n",
      "F\n",
      "            Abnormal Return    Text  Target\n",
      "Date                                       \n",
      "2011-01-03         0.000000  nannan     NaN\n",
      "2011-01-04         0.009171  nannan     NaN\n",
      "2011-01-05         0.023111  nannan     NaN\n",
      "2011-01-06         0.021089  nannan     NaN\n",
      "2011-01-07         0.005040  nannan     NaN\n"
     ]
    }
   ],
   "source": [
    "company_list = [\"005930.KS\", 'AAPL', 'INTC', 'MSFT', 'ORCL', 'SNE',\n",
    "                'TDC', 'TSLA', 'TXN', 'FB', 'AMZN', 'QCOM', 'GOOG.O',\n",
    "                'IBM', 'CVX', 'GE','WMT', 'WFC', 'XOM','T','F']\n",
    "\n",
    "news_df = combine_multiple_company(company_list)\n",
    "# news_df.to_csv('combined_companies.csv')\n",
    "# news_df = pd.read_csv('combined_companies.csv')\n",
    "\n",
    "def tag_label_to_event(news_df):\n",
    "    data = news_df.values\n",
    "    document = []\n",
    "    for row in data: \n",
    "        if row[2] == 1.0 and row[1] != 'nannan':\n",
    "            document.append( (row[1], \"pos\") )\n",
    "        elif row[2] == -1.0 and row[1] != 'nannan':\n",
    "            document.append( (row[1], \"neg\") )\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatizing¶\n",
    "Normalizing the variations of words with the same meanings into the identical word. There are two ways to normalize words which are stemming (convert to roots of words) and lemmatizing (convert to actual words). For example:\n",
    "\n",
    "- **Stemming**: The word \"positive\" and \"positively\" are derivatives of the same root of word \"positiv\" (notice \"positiv\" is non-existent word)\n",
    "- **Lemmatizing**: The word \"cats\" is converted to lemmas \"cat\" which is an actual word In this project, I use lemmatizing because I find it easier to intepret the results.\n",
    "As mentioned in Bag of Words model explaination, the classifier generally learn better with a smaller number of features in dataset. Stemming and lemmatizing is one method to reduce the number of features, thus improve the performance of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_news_data(words):\n",
    "    \"\"\"\n",
    "    Cleaning News data\n",
    "    \"\"\"\n",
    "    lst = []\n",
    "    for s in words:\n",
    "        exclude = set(string.punctuation)\n",
    "        s = ''.join(ch for ch in s if ch not in exclude)\n",
    "        sentence_token = word_tokenize(s.lower())\n",
    "        nostopword_sentence = []\n",
    "        for word_token in sentence_token:\n",
    "            stemmed_word = lemmatizer.lemmatize(word_token)\n",
    "            # stemmed_word = ps.stem(word_token)\n",
    "            if stemmed_word not in stopwords.words('english'):\n",
    "                nostopword_sentence.append(stemmed_word)\n",
    "            # if word_token not in stopwords.words('english'):\n",
    "            #     nostopword_sentence.append(word_token)\n",
    "        lst.append(nostopword_sentence)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging¶\n",
    "Speech tagging is labeling words in a sentence as nouns, adjectives, verbs...etc. The full list of tags are listed here. In this project, I decided to only keep adjectives, adverbs and verbs for our feature vectors because nouns generally don’t reveal much sentiment information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def create_lexicon(news_df):\n",
    "    text = np.array(process_news_data(news_df['Text'].values))\n",
    "    all_words = []\n",
    "    allowed_word_types = ['J','V','R']\n",
    "    for day in list(range(len(text))):\n",
    "        words = text[day]\n",
    "        pos = nltk.pos_tag(words)\n",
    "        for w in pos:\n",
    "            if w[1][0] in allowed_word_types:\n",
    "                all_words.append(w[0].lower())\n",
    "    all_words = nltk.FreqDist(all_words)\n",
    "    word_features = []\n",
    "    for i in all_words.keys():\n",
    "        if all_words.get(i) > 2:\n",
    "            word_features.append(i)\n",
    "#     word_features.remove('nannan')\n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abnormal Return</th>\n",
       "      <th>Text</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>SEOUL South Korea said on Wednesday it had app...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>SEOUL, Jan 4 Seoul shares slipped on\\rWednesda...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.0001</td>\n",
       "      <td>SEOUL Samsung Electronics, the world's top mak...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>TAIPEI Taiwanese smartphone maker HTC Corp rec...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.0001</td>\n",
       "      <td>LAS VEGAS Samsung Electronics Co unveiled its ...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Abnormal Return                                               Text  Target\n",
       "0           0.0000  SEOUL South Korea said on Wednesday it had app...     0.0\n",
       "1           0.0000  SEOUL, Jan 4 Seoul shares slipped on\\rWednesda...     0.0\n",
       "2          -0.0001  SEOUL Samsung Electronics, the world's top mak...    -1.0\n",
       "3           0.0001  TAIPEI Taiwanese smartphone maker HTC Corp rec...     1.0\n",
       "4          -0.0001  LAS VEGAS Samsung Electronics Co unveiled its ...    -1.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations and Bigram¶\n",
    "A collocation is a sequence of words that occur together unusually often (source). In a simplier sense, a collocation is a combination of words that don't carry meanings by themselves alone but is contextually meaningful together. A special case of collocation is bigrams which refer to a list of word pairs extracted from a text. For example\n",
    "\n",
    "`list(bigrams( ['more', 'is', 'said', 'than', 'done'] ))\n",
    "[('more', 'is'), ('is', 'said'), ('said', 'than'), ('than', 'done')]`  \n",
    "*than* and *done* don't provide much information to the sentence, but the pair of words than done does.\n",
    "Collocations are bigrams that appear frequently.\n",
    "\n",
    "I use Chi-square scoring function from *nltk.metrics.BigramAssocMeasures* to evaluate the performance of Bigram that I implement from nltk.metrics.BigramCollocatioinFinder. The BigramCollocationFinder maintains 2 internal Frequency Distribution, one for individual word frequencies, another for bigram frequencies. The scoring function measures the collocation correlation of 2 words, or reveal whether the bigram occurs about as frequently as each individual word.\n",
    "The Pearson's Chi-squared statistics measures the relationship between observed values and expected values or between two categorical variables. Chi-squared can be used in independence test, comparing 2 variables, or in a more general sense, test the difference in distributions of categorical variables, to see whether they are related.\n",
    "In this project, I use Chi-squared independence test to check if sequences of words, called collocations, occured together more than they might by chance. These collocations are typically names, idioms, set-phrases and the like in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_word_features(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams_tuple = bigram_finder.nbest(score_fn, n)\n",
    "    bigrams = [' '.join(each) for each in bigrams_tuple]\n",
    "    return list([ngram for ngram in itertools.chain(words, bigrams)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "singular_word_features = create_lexicon(news_df)\n",
    "word_features = bigram_word_features(singular_word_features)\n",
    "# word_features = create_lexicon(news_df)\n",
    "\n",
    "document = tag_label_to_event(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_document = open(\"pickled_algos/documents_22_abnormalReturn.pickle\",\"wb\")\n",
    "pickle.dump(document, save_document)\n",
    "save_document.close()\n",
    "\n",
    "save_word_features = open(\"pickled_algos/word_features_22_abnormalReturn.pickle\",\"wb\")\n",
    "pickle.dump(word_features, save_word_features)\n",
    "save_word_features.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(line):\n",
    "    sentence = word_tokenize(line)\n",
    "#     words = sentence\n",
    "    words = []\n",
    "    features = {}\n",
    "    for each in sentence:\n",
    "        words.append( ps.stem(each) ) \n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15024\n"
     ]
    }
   ],
   "source": [
    "featuresets = [ (find_features(line),category) for (line,category) in document]\n",
    "random.shuffle(featuresets)\n",
    "print(len(featuresets))\n",
    "\n",
    "save_features = open(\"pickled_algos/featuresets_22_abnormalReturn.pickle\",\"wb\")\n",
    "pickle.dump(featuresets, save_features)\n",
    "save_features.close()\n",
    "\n",
    "featuresets_f = open(\"pickled_algos/featuresets_22_abnormalReturn.pickle\", \"rb\")\n",
    "featuresets = pickle.load(featuresets_f)\n",
    "featuresets_f.close()\n",
    "\n",
    "posfeats = []\n",
    "negfeats = []\n",
    "for each in featuresets:\n",
    "    if each[1] == \"pos\":\n",
    "        posfeats.append(each)\n",
    "    else:\n",
    "        negfeats.append(each)\n",
    "        \n",
    "split = 0.6\n",
    "num = math.ceil(len(featuresets)* split)\n",
    "training_set = featuresets[:num]\n",
    "testing_set = featuresets[num:]\n",
    "\n",
    "negcutoff = math.ceil(len(negfeats)*split)\n",
    "poscutoff = math.ceil(len(posfeats)*split)\n",
    "\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False),0.6,96.79088592529297,0.5105674821101681,0.5053153791637137,0.5105674821101681,0.48486909214552876,1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tjhuynh/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/tjhuynh/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 63] File name too long: \"pickled_algos/LogisticRegression(C=1.0, class_weight=None, dual=True, fit_intercept=True,\\n          intercept_scaling=1, max_iter=100, multi_class='warn',\\n          n_jobs=None, penalty='l2', random_state=None, solver='warn',\\n          tol=0.0001, verbose=0, warm_start=False)_classifier.pickle\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-4a185dffd907>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mpickle_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pickled_algos/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_classifier.pickle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0msave_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0msave_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 63] File name too long: \"pickled_algos/LogisticRegression(C=1.0, class_weight=None, dual=True, fit_intercept=True,\\n          intercept_scaling=1, max_iter=100, multi_class='warn',\\n          n_jobs=None, penalty='l2', random_state=None, solver='warn',\\n          tol=0.0001, verbose=0, warm_start=False)_classifier.pickle\""
     ]
    }
   ],
   "source": [
    "algos_list = [SVC(), \n",
    "              LogisticRegression(dual = True), \n",
    "              LogisticRegression(class_weight = 'balanced'),\n",
    "              SGDClassifier(loss='log'),\n",
    "              LinearSVC(C= 0.01),\n",
    "              SVC(kernel = 'poly', C= 0.01),\n",
    "              MLPClassifier(hidden_layer_sizes=(100,100,100,100)),\n",
    "              MLPClassifier(hidden_layer_sizes=(300,300,300,300,300))]\n",
    "\n",
    "for algo in algos_list:\n",
    "    start = time.time()\n",
    "    classifier = SklearnClassifier(algo).train(trainfeats)\n",
    "    end = time.time()\n",
    "    train_time = end-start\n",
    "\n",
    "    for i, (feats, label) in enumerate(testfeats):\n",
    "        refsets[label].add(i)\n",
    "        observed = classifier.classify(feats)\n",
    "        testsets[observed].add(i)\n",
    "\n",
    "    accuracy = nltk.classify.accuracy(classifier, testfeats)\n",
    "    pos_precison = precision(refsets['pos'], testsets['pos'])\n",
    "    neg_precision = precision(refsets['neg'], testsets['neg'])\n",
    "    pos_recall = recall(refsets['pos'], testsets['pos'])\n",
    "    neg_recall = recall(refsets['neg'], testsets['neg'])\n",
    "\n",
    "    pickle_name = \"pickled_algos/\" + str(algo) + \"_classifier.pickle\"\n",
    "    save_classifier = open(pickle_name,\"wb\")\n",
    "    pickle.dump(classifier, save_classifier)\n",
    "    save_classifier.close()\n",
    "\n",
    "    algo_name = str(algo)[:-2]\n",
    "    print(str(algo)+\",\"+str(split)+\",\"+str(train_time)+\",\"\n",
    "            + str(accuracy)+\",\"+str(pos_precison)+\",\"+str(neg_precision)+\",\"\n",
    "            +str(pos_recall)+\",\"+str(neg_recall)+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy, pos_accuracy, neg_accuracy percent:  51.22316525212182 0.614233907524932 0.5876792698826597\n",
      "Most Informative Features\n",
      "                 upstart = True              pos : neg    =      6.8 : 1.0\n",
      "                   agent = True              pos : neg    =      6.0 : 1.0\n",
      "                 layaway = True              pos : neg    =      6.0 : 1.0\n",
      "                   tesco = True              pos : neg    =      6.0 : 1.0\n",
      "                 iranian = True              pos : neg    =      6.0 : 1.0\n",
      "                    mega = True              pos : neg    =      5.3 : 1.0\n",
      "                 quantum = True              pos : neg    =      5.3 : 1.0\n",
      "                 murdoch = True              neg : pos    =      5.3 : 1.0\n",
      "                    thin = True              neg : pos    =      5.3 : 1.0\n",
      "                  rupert = True              neg : pos    =      4.7 : 1.0\n",
      "                    mask = True              neg : pos    =      4.7 : 1.0\n",
      "                   grill = True              neg : pos    =      4.7 : 1.0\n",
      "                    glow = True              neg : pos    =      4.7 : 1.0\n",
      "                 malcolm = True              neg : pos    =      4.7 : 1.0\n",
      "                santiago = True              neg : pos    =      4.7 : 1.0\n",
      "                   ethic = True              pos : neg    =      4.6 : 1.0\n",
      "                     sub = True              pos : neg    =      4.6 : 1.0\n",
      "                    espn = True              pos : neg    =      4.6 : 1.0\n",
      "                 unleash = True              pos : neg    =      4.6 : 1.0\n",
      "                   naira = True              pos : neg    =      4.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Original Naive Bayes Algo accuracy, pos_accuracy, neg_accuracy percent: \", (nltk.classify.accuracy(classifier, testing_set))*100, \n",
    "    nltk.classify.accuracy(classifier,posfeats[:poscutoff]), nltk.classify.accuracy(classifier, negfeats[negcutoff:]))\n",
    "classifier.show_most_informative_features(20)\n",
    "save_classifier = open(\"pickled_algos/originalnaivebayes.pickle\",\"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB_classifier accuracy percent: 51.572641038442335\n"
     ]
    }
   ],
   "source": [
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB(alpha=3.0))\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "save_classifier = open(\"pickled_algos/BernoulliNB_classifier.pickle\",\"wb\")\n",
    "pickle.dump(BernoulliNB_classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier accuracy percent: 51.22316525212182\n"
     ]
    }
   ],
   "source": [
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression(dual = True))\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "save_classifier = open(\"pickled_algos/LogisticRegression_classifier.pickle\",\"wb\")\n",
    "pickle.dump(LogisticRegression_classifier, save_classifier)\n",
    "save_classifier.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier accuracy percent: 51.30637377267432\n"
     ]
    }
   ],
   "source": [
    "Weighted_LogisticRegression_classifier = SklearnClassifier(LogisticRegression(class_weight = 'balanced'))\n",
    "Weighted_LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(Weighted_LogisticRegression_classifier, testing_set))*100)\n",
    "save_classifier = open(\"pickled_algos/Weighted_LogisticRegression_classifier.pickle\",\"wb\")\n",
    "pickle.dump(Weighted_LogisticRegression_classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tjhuynh/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier_classifier accuracy percent: 50.29122982193377\n"
     ]
    }
   ],
   "source": [
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier(loss='log'))\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "save_classifier = open(\"pickled_algos/SGD_classifier.pickle\",\"wb\")\n",
    "pickle.dump(SGDClassifier_classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_classifier accuracy percent: 50.9069728740223\n"
     ]
    }
   ],
   "source": [
    "LinearSVC_classifier = SklearnClassifier(LinearSVC(C= 0.01))\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "save_classifier = open(\"pickled_algos/LinearSVC_classifier.pickle\",\"wb\")\n",
    "pickle.dump(LinearSVC_classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolySVC_classifier accuracy percent: 50.22466300549176\n"
     ]
    }
   ],
   "source": [
    "PolySVC_classifier = SklearnClassifier(SVC(kernel = 'poly', C= 0.01))\n",
    "PolySVC_classifier.train(training_set)\n",
    "print(\"PolySVC_classifier accuracy percent:\", (nltk.classify.accuracy(PolySVC_classifier, testing_set))*100)\n",
    "save_classifier = open(\"pickled_algos/PolySVC_classifier.pickle\",\"wb\")\n",
    "pickle.dump(PolySVC_classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_classifier accuracy percent: 50.8404060575803\n"
     ]
    }
   ],
   "source": [
    "MLP_classifier = SklearnClassifier(MLPClassifier(hidden_layer_sizes=(100,100,100,100)))\n",
    "MLP_classifier.train(training_set)\n",
    "print(\"MLP_classifier accuracy percent:\", (nltk.classify.accuracy(MLP_classifier, testing_set))*100)\n",
    "save_classifier = open(\"pickled_algos/MLP_classifier.pickle\",\"wb\")\n",
    "pickle.dump(MLP_classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big_MLP_classifier accuracy percent: 50.67398901647528\n"
     ]
    }
   ],
   "source": [
    "Big_MLP_classifier = SklearnClassifier(MLPClassifier(hidden_layer_sizes=(300,300,300,300,300)))\n",
    "Big_MLP_classifier.train(training_set)\n",
    "print(\"Big_MLP_classifier accuracy percent:\", (nltk.classify.accuracy(Big_MLP_classifier, testing_set))*100)\n",
    "save_classifier = open(\"pickled_algos/Big_MLP_classifier.pickle\",\"wb\")\n",
    "pickle.dump(Big_MLP_classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2\n",
    "## Label: S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "005930.KS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tjhuynh/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL\n",
      "INTC\n",
      "MSFT\n",
      "ORCL\n",
      "SNE\n",
      "TDC\n",
      "TSLA\n",
      "TXN\n",
      "FB\n",
      "AMZN\n",
      "QCOM\n",
      "GOOG.O\n",
      "IBM\n",
      "CVX\n",
      "GE\n",
      "VZ\n",
      "WMT\n",
      "WFC\n",
      "XOM\n",
      "T\n",
      "F\n",
      "number of word features 11972\n",
      "number of featuresets 15701\n",
      "Original Naive Bayes Algo accuracy percent:  50.955414012738856\n",
      "MNB_classifier accuracy percent: 50.955414012738856\n",
      "BernoulliNB_classifier accuracy percent: 50.98726114649682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tjhuynh/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/tjhuynh/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier accuracy percent: 50.541401273885356\n",
      "Weighted_LogisticRegression_classifier accuracy percent: 50.477707006369435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tjhuynh/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier_classifier accuracy percent: 50.41401273885351\n",
      "LinearSVC_classifier accuracy percent: 50.15923566878981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tjhuynh/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolySVC_classifier accuracy percent: 50.541401273885356\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "# import pandas_datareader.data as web\n",
    "# import datetime as dt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.classify import ClassifierI\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pickle \n",
    "import pdb\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import itertools\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas_datareader.data as web\n",
    "import datetime as dt\n",
    "import math\n",
    "# Create Word Dict (list with lenth 1644 days)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# ps = PorterStemmer()\n",
    "\n",
    "def create_news_df(news_file):\n",
    "    news_df = pd.read_csv(news_file, encoding = \"ISO-8859-1\")\n",
    "    news_df['Date'] = pd.to_datetime(news_df['Date'], errors= 'coerce')\n",
    "    news_df['Text'] = news_df['Body'].astype(str).str.cat(news_df['Title'].astype(str))\n",
    "    del news_df['Body']\n",
    "    del news_df['Title']\n",
    "    news_df.fillna('', inplace=True)\n",
    "    return news_df\n",
    "\n",
    "def create_stock_df(symbol):\n",
    "    \"\"\"\n",
    "    downloads stock which is gonna be the output of prediciton\n",
    "    \"\"\"\n",
    "    name = 'data/News/'+symbol + '_Stocks.csv'\n",
    "    out =   pd.read_csv(name, encoding = \"ISO-8859-1\")\n",
    "    # out['Date'] = pd.to_datetime(out['Date'])\n",
    "    out['Return'] = out['Adj Close'].pct_change()\n",
    "\n",
    "    sp500 = pd.read_csv('data/GuruFocus/Yahoo_Index_GSPC.csv')\n",
    "    sp500['Date'] = pd.to_datetime(sp500['Date'])\n",
    "    sp500['Sp_Return'] = sp500['Adj Close'].pct_change()\n",
    "\n",
    "    del sp500['Open']\n",
    "    del sp500['Close']\n",
    "    del sp500['High']\n",
    "    del sp500['Low']\n",
    "    del sp500['Volume']\n",
    "    del sp500['Adj Close']\n",
    "\n",
    "    df= out.set_index('Date').join(sp500.set_index('Date'))\n",
    "    df = df.dropna()\n",
    "\n",
    "    df['Difference'] = df['Return'] - df['Sp_Return']\n",
    "    return df\n",
    "\n",
    "def combine_final_df(news_df, stock_df):\n",
    "    df= stock_df.join(news_df.set_index('Date'))\n",
    "    df.fillna(0, inplace = True)\n",
    "    df['Target'] = np.nan\n",
    "    requirement = 0.0\n",
    "    for i in range(len(df)):\n",
    "        if df['Difference'].iloc[i] > requirement:\n",
    "            df['Target'].iloc[i] = 1.0\n",
    "        elif df['Difference'].iloc[i] <  requirement:\n",
    "            df['Target'].iloc[i] = -1.0\n",
    "        else:\n",
    "            df['Target'].iloc[i] = 0.0\n",
    "    return df\n",
    "\n",
    "def combine_multiple_company(company_list):  \n",
    "    \"\"\"\n",
    "    Concatenate multiple dataframe of companies together to create big lexicon\n",
    "    \"\"\"\n",
    "    company_dfs = []\n",
    "\n",
    "    for company in company_list:\n",
    "        print(company)\n",
    "        news_file_name = 'data/News/'+ company + '_News.csv'\n",
    "        news = create_news_df(news_file_name)\n",
    "        \n",
    "        stock = create_stock_df(company)\n",
    "\n",
    "        final = combine_final_df(news, stock)\n",
    "        company_dfs.append(final)\n",
    "    total = pd.concat(company_dfs, ignore_index = True)\n",
    "    return total\n",
    "\n",
    "company_list = [\"005930.KS\", 'AAPL', 'INTC', 'MSFT', 'ORCL', 'SNE',\n",
    "                'TDC', 'TSLA', 'TXN', 'FB', 'AMZN', 'QCOM', 'GOOG.O',\n",
    "                'IBM', 'CVX', 'GE', 'VZ','WMT', 'WFC', 'XOM','T','F']\n",
    "\n",
    "# company_list = [\"005930.KS\", 'AAPL']              \n",
    "\n",
    "news_df = combine_multiple_company(company_list)\n",
    "\n",
    "def tag_label_to_event(news_df):\n",
    "    data = news_df.values\n",
    "    document = []\n",
    "    for row in data: \n",
    "        if row[-1] == 1.0 and row[-2] != 'nannan':\n",
    "            document.append( (row[-2], \"pos\") )\n",
    "        elif row[-1] == -1.0 and row[-2] != 'nannan':\n",
    "            document.append( (row[-2], \"neg\") )\n",
    "    return document\n",
    "\n",
    "def process_news_data(words):\n",
    "    \"\"\"\n",
    "    Cleaning News data\n",
    "    \"\"\"\n",
    "    lst = []\n",
    "    for s in words:\n",
    "        exclude = set(string.punctuation)\n",
    "        s = ''.join(ch for ch in s if ch not in exclude)\n",
    "        sentence_token = word_tokenize(s.lower())\n",
    "        nostopword_sentence = []\n",
    "        for word_token in sentence_token:\n",
    "\n",
    "            stemmed_word = lemmatizer.lemmatize(word_token)\n",
    "            # stemmed_word = ps.stem(word_token)\n",
    "            if stemmed_word not in stopwords.words('english'):\n",
    "                nostopword_sentence.append(stemmed_word)\n",
    "            # if word_token not in stopwords.words('english'):\n",
    "            #     nostopword_sentence.append(word_token)\n",
    "        lst.append(nostopword_sentence)\n",
    "    return lst\n",
    "    \n",
    "def create_lexicon(news_df):\n",
    "    text = np.array(process_news_data(news_df['Text'].values))\n",
    "    all_words = []\n",
    "    allowed_word_types = ['J','V','R']\n",
    "    for day in list(range(len(text))):\n",
    "        words = text[day]\n",
    "        pos = nltk.pos_tag(words)\n",
    "        for w in pos:\n",
    "            if w[1][0] in allowed_word_types:\n",
    "                all_words.append(w[0].lower())\n",
    "    all_words = nltk.FreqDist(all_words)\n",
    "    word_features = []\n",
    "    for i in all_words.keys():\n",
    "        if all_words.get(i) > 2:\n",
    "            word_features.append(i)\n",
    "    # word_features.remove('nannan')\n",
    "    return word_features\n",
    "\n",
    "def bigram_word_features(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams_tuple = bigram_finder.nbest(score_fn, n)\n",
    "    bigrams = [' '.join(each) for each in bigrams_tuple]\n",
    "    return list([ngram for ngram in itertools.chain(words, bigrams)])\n",
    "\n",
    "\n",
    "singular_word_features = create_lexicon(news_df)\n",
    "word_features = bigram_word_features(singular_word_features)\n",
    "# word_features = create_lexicon(news_df)\n",
    "print('number of word features', len(word_features))\n",
    "\n",
    "document = tag_label_to_event(news_df)\n",
    "\n",
    "save_document = open(\"pickled_algos/documents_22_sp500.pickle\",\"wb\")\n",
    "pickle.dump(document, save_document)\n",
    "save_document.close()\n",
    "\n",
    "save_word_features = open(\"pickled_algos/word_features_22_sp500.pickle\",\"wb\")\n",
    "pickle.dump(word_features, save_word_features)\n",
    "save_word_features.close()\n",
    "\n",
    "def find_features(line):\n",
    "    sentence = word_tokenize(line)\n",
    "    # words = sentence\n",
    "    words = []\n",
    "    for each in sentence:\n",
    "        words.append( lemmatizer.lemmatize(each) ) \n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n",
    "\n",
    "featuresets = [ (find_features(line),category) for (line,category) in document]\n",
    "random.shuffle(featuresets)\n",
    "print('number of featuresets', len(featuresets))\n",
    "\n",
    "save_features = open(\"pickled_algos/featuresets_22_sp500.pickle\",\"wb\")\n",
    "pickle.dump(featuresets, save_features)\n",
    "save_features.close()\n",
    "\n",
    "# featuresets_f = open(\"pickled_algos/featuresets_22_sp500.pickle\", \"rb\")\n",
    "# featuresets = pickle.load(featuresets_f)\n",
    "# featuresets_f.close()\n",
    "\n",
    "num = math.ceil(len(featuresets)*0.80)\n",
    "training_set = featuresets[:num]\n",
    "testing_set = featuresets[num:]\n",
    "\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Original Naive Bayes Algo accuracy percent: \", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "# classifier.show_most_informative_features(20)\n",
    "save_classifier = open(\"pickled_algos/sp500_originalnaivebayes.pickle\",\"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "save_classifier = open(\"pickled_algos/sp500_MNB_classifier.pickle\",\"wb\")\n",
    "pickle.dump(MNB_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "save_classifier = open(\"pickled_algos/sp500_BernoulliNB_classifier.pickle\",\"wb\")\n",
    "pickle.dump(BernoulliNB_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression(dual = True))\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "save_classifier = open(\"pickled_algos/sp500_LogisticRegression_classifier.pickle\",\"wb\")\n",
    "pickle.dump(LogisticRegression_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "Weighted_LogisticRegression_classifier = SklearnClassifier(LogisticRegression(class_weight = 'balanced'))\n",
    "Weighted_LogisticRegression_classifier.train(training_set)\n",
    "print(\"Weighted_LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(Weighted_LogisticRegression_classifier, testing_set))*100)\n",
    "save_classifier = open(\"pickled_algos/sp500_Weighted_LogisticRegression_classifier.pickle\",\"wb\")\n",
    "pickle.dump(Weighted_LogisticRegression_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "save_classifier = open(\"pickled_algos/sp500_SGD_classifier.pickle\",\"wb\")\n",
    "pickle.dump(SGDClassifier_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "save_classifier = open(\"pickled_algos/sp500_LinearSVC_classifier.pickle\",\"wb\")\n",
    "pickle.dump(LinearSVC_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "PolySVC_classifier = SklearnClassifier(SVC(kernel = 'poly', C= 0.01))\n",
    "PolySVC_classifier.train(training_set)\n",
    "print(\"PolySVC_classifier accuracy percent:\", (nltk.classify.accuracy(PolySVC_classifier, testing_set))*100)\n",
    "save_classifier = open(\"pickled_algos/sp500_PolySVC_classifier.pickle\",\"wb\")\n",
    "pickle.dump(PolySVC_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "RbfSVC_classifier = SklearnClassifier(SVC(C= 0.01))\n",
    "RbfSVC_classifier.train(training_set)\n",
    "print(\"RbfSVC_classifier accuracy percent:\", (nltk.classify.accuracy(RbfSVC_classifier, testing_set))*100)\n",
    "save_classifier = open(\"pickled_algos/sp500_RbfSVC_classifier.pickle\",\"wb\")\n",
    "pickle.dump(RbfSVC_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC(kernel = 'poly'))\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)\n",
    "save_classifier = open(\"pickled_algos/sp500_NuSVC_classifier.pickle\",\"wb\")\n",
    "pickle.dump(NuSVC_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "MLP_classifier = SklearnClassifier(MLPClassifier(hidden_layer_sizes=(100,100,100,100)))\n",
    "MLP_classifier.train(training_set)\n",
    "print(\"MLP_classifier accuracy percent:\", (nltk.classify.accuracy(MLP_classifier, testing_set))*100)\n",
    "save_classifier = open(\"pickled_algos/sp500_MLP_classifier.pickle\",\"wb\")\n",
    "pickle.dump(MLP_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "Big_MLP_classifier = SklearnClassifier(MLPClassifier(hidden_layer_sizes=(300,300,300,300,300)))\n",
    "Big_MLP_classifier.train(training_set)\n",
    "print(\"Big_MLP_classifier accuracy percent:\", (nltk.classify.accuracy(Big_MLP_classifier, testing_set))*100)\n",
    "save_classifier = open(\"pickled_algos/sp500_Big_MLP_classifier.pickle\",\"wb\")\n",
    "pickle.dump(Big_MLP_classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
